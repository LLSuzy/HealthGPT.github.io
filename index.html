<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="HealthGPT: A Medical Large Vision-Language Model.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>HealthGPT: A Medical Large Vision-Language Model</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/HealthGPT.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>

<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">HealthGPT : A Medical Large Vision-Language Model for Unifying Comprehension and Generation via Heterogeneous Knowledge Adaptation</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                Tianwei Lin<sup><span style="color:#ed4b82;">1</span></sup>,
              </span>
              <span class="author-block">
                Wenqiao Zhang<sup style="color:#ed4b82;">1</sup>,</span>
              <span class="author-block">
                Sijing Li<sup style="color:#ed4b82;">1</sup>,
              </span>
              <span class="author-block">
                Yuqian Yuan<sup style="color:#ed4b82;">1</sup>,
              </span>
              <span class="author-block">
                Binhe Yu<sup><span style="color:#efd373;">2</span></sup>,
              </span>
              <span class="author-block">
                Haoyuan Li<sup style="color:#4c58da;">3</sup>,
              </span>
              <span class="author-block">
                Wanggui He<sup style="color:#4c58da;">3</sup>,
              </span>
              <span class="author-block">
                Hao Jiang<sup style="color:#4c58da;">3</sup>,
              </span>
              <span class="author-block">
                Mengze Li<sup style="color:#7d32ac;">4</sup>,
              </span>
              <span class="author-block">
                 Xiaohui Song<sup style="color:#ed4b82;">1</sup>,
              </span>
              <span class="author-block">
                Siliang Tang<sup style="color:#ed4b82;">1</sup>,
              </span>
              <span class="author-block">
                Jun Xiao<sup style="color:#ed4b82;">1</sup>,
              </span>
              <span class="author-block">
                Hui Lin<sup style="color:#ed4b82;">1</sup>,
              </span>
              <span class="author-block">
                Yueting Zhuang<sup style="color:#ed4b82;">1</sup>,
              </span>
              <span class="author-block">
                Beng Chin Ooi<sup style="color:#4beda9;">5</sup>,
              </span>
            </div>

            <br>
            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup style="color:#ed4b82;">1</sup>Zhejiang University</span>
              <span class="author-block"><sup style="color:#efd373;">2</sup>University of Electronic Science and Technology of China</span>
              <br>
              <span class="author-block"><sup style="color:#4c58da;">3</sup>Alibaba</span>
              <span class="author-block"><sup style="color:#7d32ac;">4</sup>The Hong Kong University of Science and Technology</span>
              <span class="author-block"><sup style="color:#4beda9;">5</sup>National University of Singapore</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2502.09838" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://github.com/DCDmllm/HealthGPT?tab=readme-ov-file" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                <!-- Dataset Link. -->

                  </a>
              </div>
              <span class="link-block">
                  <a href=""
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="far fa-images"></i>
                    </span>
                    <span>Dataset(Comming soon)</span>
              <span class="link-block">
                <a href="https://huggingface.co/lintw/HealthGPT-M3"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <!-- <i class="far fa-images"></i> -->
                      <p style="font-size:18px">ðŸ¤—</p>
                  </span>
                  <span>Model</span>
                </a>
              </span> 

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">ðŸ”¥Highlights</h2>
          <div class="content has-text-justified">
            <p>
              In this work, we design <b><i>HealthGPT</i></b> - a large visual language model (Med-LVLM) for the medical domain that integrates medical visual comprehension and generation in a unified autoregressive paradigm. Our concept is to progressively apply heterogeneous comprehension and generation knowledge to pre-trained Large Language Models (LLMs) so that the pre-trained LLMs can effectively follow visual comprehension and generation instructions. We contend that achieving this necessitates three essential components.
            </p>
            <p>
              1. <b>Dataset.</b> Firstly,  in order to effectively train HealthGPT, we designed a comprehensive medical domain-specific comprehension and generation dataset called  <b>VL-Health.</b>
            </p>
            <p>
              2. <b>Method.</b> Next, HealthGPT has a unified framework that integrates medical visual understanding and generation.
              It achieves this through a novel <b>Heterogeneous Low Rank Adaptive (H-LoRA)</b> technique complemented by a tailored
              <b>Hierarchical Visual Perception Approach</b> and a <b>three-stage learning strategy.</b>
            </p>
            <p>
              3. <b>Models.</b> We released two configurations of models, HealthGPT-M3 and HealthGPT-L14,
              to meet different requirements and resource availability. <b>HealthGPT-M3</b> is a smaller version optimized
              for speed and reduced memory usage, while <b>HealthGPT-L14</b> is a larger version designed for higher performance and more complex tasks.
            </p>
          </div>
        </div>
      </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              We present HealthGPT, a powerful Medical Large Vision-Language Model (Med-LVLM) that integrates medical visual
              comprehension and generation capabilities within a unified autoregressive paradigm. Our bootstrapping philosophy is
              to progressively adapt heterogeneous comprehension and generation knowledge to pre-trained large language models (LLMs).
              This is achieved through a novel heterogeneous low-rank adaptation (H-LoRA) technique, which is complemented by a
              tailored hierarchical visual perception approach and a three-stage learning strategy. To effectively learn the HealthGPT,
              we devise a comprehensive medical domain-specific comprehension and generation dataset called VL-Health. Experimental
              results demonstrate exceptional performance and scalability of HealthGPT in medical visual unified tasks.
              Our project can be accessed at this <a href="https://github.com/DCDmllm/HealthGPT">https URL.</a>
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="hero is-light is-small">
    <div class="hero-body has-text-centered">
    <h1 class="title is-1 mathvista_other">
      <span class="mathvista_other" style="vertical-align: middle">Task Classification and Support</span>
    </h1>
    </div>
  </section>
              

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered ">
        <div class="column is-full-width">
          <div class="column is-centered ">
            <img src="./static/images/intro.png" class="interpolation-image" alt="Interpolate start reference image." />

            <p class="has-text-centered">
              <br>
              <b>HealthGPT</b> supports <b>7</b> types of medical comprehension tasks and <b>5</b> types of medical generation tasks, outperforming recent unified visual models and medical-specific models.
            </p>
          </div>
        </div>
      </div>
    </div>
    <!-- </div> -->
  </section>


  <section class="hero is-light is-small">
    <div class="hero-body has-text-centered">
    <h1 class="title is-1 mathvista_other">
      <span class="mathvista_other" style="vertical-align: middle">HealthGPT Model Architecture</span>
    </h1>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered ">
        <div class="column is-full-width">
          <div class="column is-centered ">
            <img src="./static/images/Framework.png" class="interpolation-image" alt="Interpolate start reference image." />
            <p class="has-text-centered">
              <br>
              The HealthGPT architecture integrates <b>hierarchical visual perception</b> and <b>H-LoRA</b>, employing a task-specific hard router to select visual features and H-LoRA plugins, generating text and vision outputs with an autoregressive manner.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>




  <section class="hero is-light is-small">
    <div class="hero-body has-text-centered">
    <h1 class="title is-1 mathvista_other">
      <span class="mathvista_other" style="vertical-align: middle">Main Results</span>
    </h1>
    </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">
      <div class="columns is-centered ">
        <div class="column is-full-width">
          <div class="column is-centered ">
            <p class="has-text-centered">
              Table 1: Comparison of HealthGPT with other LVLMs and unified multi-modal models on medical visual comprehension
              tasks. <b>Bold</b> and <u>underlined</u> text indicates the best performance and second-best performance, respectively.
            </p>
            <img src="./static/images/r1.png" class="interpolation-image" alt="Interpolate start reference image." />
          </div>
          <div class="column is-centered ">
            <p class="has-text-centered">
              Table 2: The experimental results for the four modality conversion tasks.
            </p>
            <img src="./static/images/r2.png" class="interpolation-image" alt="Interpolate start reference image." />
          </div>
          <div class="column is-centered " style="text-align: center;">
            <p class="has-text-centered">
              Table 3: Comparison results of super-resolution task.
            </p>
            <img src="./static/images/img.png" class="interpolation-image" alt="Interpolate start reference image." />
          </div>
        </div>
      </div>
    </div>
  </section>


  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{lin2025healthgptmedicallargevisionlanguage,
      title={HealthGPT: A Medical Large Vision-Language Model for Unifying Comprehension and Generation via Heterogeneous Knowledge Adaptation},
      author={Tianwei Lin and Wenqiao Zhang and Sijing Li and Yuqian Yuan and Binhe Yu and Haoyuan Li and Wanggui He and Hao Jiang and Mengze Li and Xiaohui Song and Siliang Tang and Jun Xiao and Hui Lin and Yueting Zhuang and Beng Chin Ooi},
      year={2025},
      eprint={2502.09838},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2502.09838},
}</code></pre>
    </div>
  </section>


</body>

</html>
